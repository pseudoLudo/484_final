{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ECSE 484 Final project - Text summarizer\n",
    "\n",
    "<i>Ruthvik thanda - rxt309</i>\n",
    "\n",
    "This project utilises the amazon food reviews dataset\n",
    "\n",
    "Source : https://www.kaggle.com/snap/amazon-fine-food-reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 479,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 480,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('Reviews.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 481,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>ProductId</th>\n",
       "      <th>UserId</th>\n",
       "      <th>ProfileName</th>\n",
       "      <th>HelpfulnessNumerator</th>\n",
       "      <th>HelpfulnessDenominator</th>\n",
       "      <th>Score</th>\n",
       "      <th>Time</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>B001E4KFG0</td>\n",
       "      <td>A3SGXH7AUHU8GW</td>\n",
       "      <td>delmartian</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1303862400</td>\n",
       "      <td>Good Quality Dog Food</td>\n",
       "      <td>I have bought several of the Vitality canned d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>B00813GRG4</td>\n",
       "      <td>A1D87F6ZCVE5NK</td>\n",
       "      <td>dll pa</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1346976000</td>\n",
       "      <td>Not as Advertised</td>\n",
       "      <td>Product arrived labeled as Jumbo Salted Peanut...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>B000LQOCH0</td>\n",
       "      <td>ABXLMWJIXXAIN</td>\n",
       "      <td>Natalia Corres \"Natalia Corres\"</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1219017600</td>\n",
       "      <td>\"Delight\" says it all</td>\n",
       "      <td>This is a confection that has been around a fe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>B000UA0QIQ</td>\n",
       "      <td>A395BORC6FGVXV</td>\n",
       "      <td>Karl</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1307923200</td>\n",
       "      <td>Cough Medicine</td>\n",
       "      <td>If you are looking for the secret ingredient i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>B006K2ZZ7K</td>\n",
       "      <td>A1UQRSCLF8GW1T</td>\n",
       "      <td>Michael D. Bigham \"M. Wassir\"</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1350777600</td>\n",
       "      <td>Great taffy</td>\n",
       "      <td>Great taffy at a great price.  There was a wid...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id   ProductId          UserId                      ProfileName  \\\n",
       "0   1  B001E4KFG0  A3SGXH7AUHU8GW                       delmartian   \n",
       "1   2  B00813GRG4  A1D87F6ZCVE5NK                           dll pa   \n",
       "2   3  B000LQOCH0   ABXLMWJIXXAIN  Natalia Corres \"Natalia Corres\"   \n",
       "3   4  B000UA0QIQ  A395BORC6FGVXV                             Karl   \n",
       "4   5  B006K2ZZ7K  A1UQRSCLF8GW1T    Michael D. Bigham \"M. Wassir\"   \n",
       "\n",
       "   HelpfulnessNumerator  HelpfulnessDenominator  Score        Time  \\\n",
       "0                     1                       1      5  1303862400   \n",
       "1                     0                       0      1  1346976000   \n",
       "2                     1                       1      4  1219017600   \n",
       "3                     3                       3      2  1307923200   \n",
       "4                     0                       0      5  1350777600   \n",
       "\n",
       "                 Summary                                               Text  \n",
       "0  Good Quality Dog Food  I have bought several of the Vitality canned d...  \n",
       "1      Not as Advertised  Product arrived labeled as Jumbo Salted Peanut...  \n",
       "2  \"Delight\" says it all  This is a confection that has been around a fe...  \n",
       "3         Cough Medicine  If you are looking for the secret ingredient i...  \n",
       "4            Great taffy  Great taffy at a great price.  There was a wid...  "
      ]
     },
     "execution_count": 481,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 482,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    568454.000000\n",
       "mean         81.005522\n",
       "std          80.807102\n",
       "min           2.000000\n",
       "25%          33.000000\n",
       "50%          57.000000\n",
       "75%          99.000000\n",
       "max        3525.000000\n",
       "Name: revLength, dtype: float64"
      ]
     },
     "execution_count": 482,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = data[['Summary','Text']]\n",
    "data['revLength'] = data['Text'].str.count(' ')\n",
    "data['revLength'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 483,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    568427.000000\n",
       "mean          3.128462\n",
       "std           2.619420\n",
       "min           0.000000\n",
       "25%           1.000000\n",
       "50%           3.000000\n",
       "75%           4.000000\n",
       "max          41.000000\n",
       "Name: sumLen, dtype: float64"
      ]
     },
     "execution_count": 483,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['sumLen'] = data['Summary'].str.count(' ')\n",
    "data['sumLen'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from the data description,picking a set with smaller size so that the model is faster to build and easier to verify on smaller texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 484,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.loc[data['sumLen']<8]\n",
    "data = data.loc[data['revLength']<30]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "preprocessing and removing punctuations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 485,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['textLowercase'] = data['Text'].str.lower()\n",
    "data['textNopunc'] = data['textLowercase'].str.replace('[^\\w\\s]','')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 486,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['summaryLowercase'] = data[\"Summary\"].str.lower()\n",
    "data['summaryNopunc'] =  '_begin_' + ' ' +data['summaryLowercase'].str.replace('[^\\w\\s]','')+ ' ' +'_end_'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating text and summary tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 487,
   "metadata": {},
   "outputs": [],
   "source": [
    "textFeature = 5000\n",
    "textLen = 30\n",
    "\n",
    "textTokens = tf.keras.preprocessing.text.Tokenizer(num_words=textFeature) \n",
    "textTokens.fit_on_texts(list(data['textNopunc'].astype(str))) \n",
    "textSeq =textTokens.texts_to_sequences(list(data['textNopunc'].astype(str)))\n",
    "textSeq =tf.keras.preprocessing.sequence.pad_sequences(textSeq, maxlen=textLen)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 488,
   "metadata": {},
   "outputs": [],
   "source": [
    "summaryFeature = 5000\n",
    "summaryLen = 8\n",
    "\n",
    "summaryTokens = tf.keras.preprocessing.text.Tokenizer(num_words=summaryFeature, filters = '*') \n",
    "summaryTokens.fit_on_texts(list(data['summaryNopunc'].astype(str))) \n",
    "summarySeq = summaryTokens.texts_to_sequences(list(data['summaryNopunc'].astype(str)))\n",
    "summarySeq = tf.keras.preprocessing.sequence.pad_sequences(summarySeq, maxlen=summaryLen, padding ='post') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 489,
   "metadata": {},
   "outputs": [],
   "source": [
    "summaryVector = summarySeq\n",
    "decoderData = summaryVector[:, :-1]\n",
    "decoderTarget = summaryVector[:, 1:]\n",
    "textVector = textSeq\n",
    "encoderData = textVector\n",
    "length = encoderData.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 490,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoderVocab = len(textTokens.word_index) + 1 \n",
    "decoderVocab = len(summaryTokens.word_index) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 491,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create encoder and decoder with TF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 492,
   "metadata": {},
   "outputs": [],
   "source": [
    "#encoder\n",
    "encoderInp = tf.keras.Input(shape=(length,), name='Encoder')\n",
    "A = tf.keras.layers.Embedding(encoderVocab, dim, name='BWEmbed', mask_zero=False)(encoderInp)\n",
    "A = tf.keras.layers.BatchNormalization(name='encoderBN1')(A)\n",
    "\n",
    "_, hidden_state = tf.keras.layers.GRU(dim, return_state=True, name='EGRU')(A)\n",
    "\n",
    "eModel = tf.keras.Model(inputs=encoderInp, outputs=hidden_state, name='EModel')\n",
    "eOut = eModel(encoderInp)\n",
    "\n",
    "#Decoder\n",
    "decoderInp = tf.keras.Input(shape=(None,), name='Decoder')  \n",
    "dEmbedd = tf.keras.layers.Embedding(decoderVocab, dim, name='DEmbedding', mask_zero=False)(decoderInp)\n",
    "\n",
    "dBN = tf.keras.layers.BatchNormalization(name='decoderBN1')(dEmbedd)\n",
    "dGru = tf.keras.layers.GRU(dim, return_state=True, return_sequences=True, name='Decoder-GRU')\n",
    "dGruOp, _ = dGru(dBN, initial_state=eOut) \n",
    "A = tf.keras.layers.BatchNormalization(name='decoderBN2')(dGruOp)\n",
    "\n",
    "#Dense layer\n",
    "dense = tf.keras.layers.Dense(decoderVocab, activation='softmax', name='OP')\n",
    "dOP = dense(A)\n",
    "\n",
    "#Seq2Seq\n",
    "seq2seq = tf.keras.Model([encoderInp, decoderInp], dOP)\n",
    "seq2seq.compile(optimizer=tf.keras.optimizers.Nadam(lr=0.001), loss='sparse_categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 493,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_71\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "Decoder (InputLayer)            [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "DEmbedding (Embedding)          (None, None, 50)     692150      Decoder[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "Encoder (InputLayer)            [(None, 30)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "decoderBN1 (BatchNormalization) (None, None, 50)     200         DEmbedding[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "EModel (Functional)             (None, 50)           2013300     Encoder[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "Decoder-GRU (GRU)               [(None, None, 50), ( 15300       decoderBN1[0][0]                 \n",
      "                                                                 EModel[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "decoderBN2 (BatchNormalization) (None, None, 50)     200         Decoder-GRU[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "OP (Dense)                      (None, None, 13843)  705993      decoderBN2[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 3,427,143\n",
      "Trainable params: 3,426,843\n",
      "Non-trainable params: 300\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "seq2seq.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 494,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "1449/1449 [==============================] - 405s 279ms/step - loss: 2.6741 - val_loss: 1.9822\n",
      "Epoch 2/3\n",
      "1449/1449 [==============================] - 435s 300ms/step - loss: 1.7993 - val_loss: 1.7448\n",
      "Epoch 3/3\n",
      "1449/1449 [==============================] - 413s 285ms/step - loss: 1.5451 - val_loss: 1.6534\n"
     ]
    }
   ],
   "source": [
    "batch = 64\n",
    "epoch_no = 3 \n",
    "mod = seq2seq.fit([encoderData, decoderData], np.expand_dims(decoderTarget, -1),\n",
    "          batch_size=batch,  epochs=epoch_no ,  validation_split=0.15) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing,Place text you want to summarize in the test list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 532,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = ['definitely not worth buying flavored water with a few teaspoons of beans and rice that doesnt taste like normal beans and rice i wont ever buy this again']\n",
    "\n",
    "textTokens.fit_on_texts(test)\n",
    "testTokens = textTokens.texts_to_sequences(test)\n",
    "testTokens = tf.keras.preprocessing.sequence.pad_sequences(testTokens, maxlen=textLen)\n",
    "encoding = eModel.predict(testTokens) \n",
    "dim = seq2seq.get_layer('DEmbedding').output_shape[-1]\n",
    "\n",
    "decoderInp = seq2seq.get_layer('Decoder').input \n",
    "dEmbedd = seq2seq.get_layer('DEmbedding')(decoderInp)\n",
    "dBN = seq2seq.get_layer('decoderBN1')(dEmbedd)\n",
    "gruInp = tf.keras.Input(shape=(dim,), name='hidden_state_input')\n",
    "gruOut, gruOutState = seq2seq.get_layer('Decoder-GRU')([dBN, gruInp])\n",
    "dBN2 = seq2seq.get_layer('decoderBN2')(gruOut)\n",
    "dOut = seq2seq.get_layer('OP')(dBN2)\n",
    "dModel = tf.keras.Model([decoderInp, gruInp],[dOut, gruOutState])\n",
    "oEncoding = encoding\n",
    "startState = np.array(summaryTokens.word_index['_begin_']).reshape(1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "prediction and append into sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 533,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:6 out of the last 15 calls to <function Model.make_predict_function.<locals>.predict_function at 0x000001B0185E1D30> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x000001B0185E1D30> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    }
   ],
   "source": [
    "sentence = []\n",
    "flag = False\n",
    "resVocab = dict((v, k) for k, v in summaryTokens.word_index.items())\n",
    "\n",
    "while not flag:\n",
    "    p, s = dModel.predict([startState, encoding])\n",
    "\n",
    "    p_i = np.argmax(p[:, :, 2:]) + 2\n",
    "    word = resVocab[p_i]\n",
    "    if word == '_end_' or len(sentence) >= summaryLen:\n",
    "        flag = True\n",
    "        break\n",
    "    sentence.append(word)\n",
    "\n",
    "    encoding = s\n",
    "    startState = np.array(p_i).reshape(1, 1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 534,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "not as good as the flavor \n"
     ]
    }
   ],
   "source": [
    "result=\"\"\n",
    "for i in sentence:\n",
    "    result+=(i)\n",
    "    result+=(\" \")\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
